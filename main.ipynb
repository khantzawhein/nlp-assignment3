{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.107134Z",
     "start_time": "2025-02-20T08:27:46.566620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "# Load the English model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "data = pd.read_csv(\"work-data/train.csv\")\n",
    "data.dropna(inplace=True)\n",
    "data.head()\n",
    "\n",
    "data[\"tokens\"] = None"
   ],
   "id": "c26e56143fe95a20",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.111218Z",
     "start_time": "2025-02-20T08:27:47.109275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # text = spell(text)\n",
    "    # replace it 's with is\n",
    "    text = text.replace(r\"'s\", \"is\")\n",
    "    # replace 've with have\n",
    "    text = text.replace(\"'ve\", \"have\")\n",
    "    # replace n't with not\n",
    "    text = text.replace(\"n't\", \"not\")\n",
    "    # replace 're with are\n",
    "    text = text.replace(\"'re\", \"are\")\n",
    "    # replace 'd with would\n",
    "    text = text.replace(\"'d\", \"would\")\n",
    "    # replace 'll with will\n",
    "    text = text.replace(\"'ll\", \"will\")\n",
    "    # replace 'm with am\n",
    "    text = text.replace(\"'m\", \"am\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":  # Detect person names\n",
    "            text = text.replace(ent.text, \"person\")\n",
    "    doc = nlp(text)\n",
    "    # remove extra spaces\n",
    "    doc = [token for token in doc if\n",
    "           not token.is_space and token.is_ascii and token.is_alpha and not token.is_currency and not token.is_digit and not token.is_punct]\n",
    "    return ' '.join([token.text for token in doc]).lower()\n",
    "    # return ' '.join([token.lemma_ for token in doc\n",
    "    #                  if token.lemma_ != '-PRON-']).lower()\n"
   ],
   "id": "ff4f6a4159831e06",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.124051Z",
     "start_time": "2025-02-20T08:27:47.116963Z"
    }
   },
   "cell_type": "code",
   "source": "preprocess_text(\"I 'm running to the garden James Madison\")",
   "id": "fb329b884e80951d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am running to the garden person'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.131368Z",
     "start_time": "2025-02-20T08:27:47.129853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sentiment_features(text):\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            \"polarity\": blob.sentiment.polarity,\n",
    "            \"subjectivity\": blob.sentiment.subjectivity\n",
    "        }\n",
    "    except:\n",
    "        return {\"polarity\": 0, \"subjectivity\": 0}\n"
   ],
   "id": "96fc5eea2c2253a3",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.141497Z",
     "start_time": "2025-02-20T08:27:47.139669Z"
    }
   },
   "cell_type": "code",
   "source": "get_sentiment_features(\"textblob is amazingly simple to use. What great fun\")",
   "id": "de1ae7bf54f89be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 0.3666666666666667, 'subjectivity': 0.4357142857142857}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.148999Z",
     "start_time": "2025-02-20T08:27:47.147158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text_stats(text):\n",
    "    \"\"\"Calculate statistical features from text\"\"\"\n",
    "    words = text.split()\n",
    "    sentiment = get_sentiment_features(text)\n",
    "    return {\n",
    "        'text_length': len(text),\n",
    "        'word_count': len(words),\n",
    "        'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "        'sentiment_polarity': sentiment['polarity'],\n",
    "        'sentiment_subjectivity': sentiment['subjectivity'],\n",
    "        'unique_words': len(set(words)),\n",
    "        'stopword_count': len([word for word in words if word in nlp.Defaults.stop_words]),\n",
    "    }"
   ],
   "id": "787305f5bf6fa681",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.155845Z",
     "start_time": "2025-02-20T08:27:47.153973Z"
    }
   },
   "cell_type": "code",
   "source": "get_text_stats(\"Textblob is amazingly simple to use. What great fun!\")",
   "id": "1cf75d472b8bdaea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_length': 52,\n",
       " 'word_count': 9,\n",
       " 'avg_word_length': 4.888888888888889,\n",
       " 'sentiment_polarity': 0.39166666666666666,\n",
       " 'sentiment_subjectivity': 0.4357142857142857,\n",
       " 'unique_words': 9,\n",
       " 'stopword_count': 2}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.167213Z",
     "start_time": "2025-02-20T08:27:47.165773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text_stats_value(X):\n",
    "    features_list = []\n",
    "    for text in X:\n",
    "        stats = get_text_stats(text)\n",
    "        features = list(stats.values())\n",
    "        features_list.append(features)\n",
    "    return np.array(features_list)"
   ],
   "id": "c2a2d27a52ee0ea8",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:27:47.183967Z",
     "start_time": "2025-02-20T08:27:47.181792Z"
    }
   },
   "cell_type": "code",
   "source": "get_text_stats_value([\"Textblob is amazingly simple to use. What great fun!\"])",
   "id": "af09de300a9db065",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52.        ,  9.        ,  4.88888889,  0.39166667,  0.43571429,\n",
       "         9.        ,  2.        ]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:28:35.273841Z",
     "start_time": "2025-02-20T08:27:47.205102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"work-data/train.csv\")\n",
    "print(\"Starting text preprocessing...\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Text preprocessing completed.\")"
   ],
   "id": "287ac2efa4b004fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text preprocessing...\n",
      "Text preprocessing completed.\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:28:35.742533Z",
     "start_time": "2025-02-20T08:28:35.289437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_features = get_text_stats_value(df['processed_text'])\n",
    "\n",
    "text_features"
   ],
   "id": "5d60e568407043b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[168.        ,  25.        ,   5.76      , ...,   0.60833333,\n",
       "         25.        ,  11.        ],\n",
       "       [151.        ,  21.        ,   6.23809524, ...,   0.39444444,\n",
       "         20.        ,   6.        ],\n",
       "       [176.        ,  33.        ,   4.36363636, ...,   0.74285714,\n",
       "         28.        ,  21.        ],\n",
       "       ...,\n",
       "       [ 50.        ,  13.        ,   2.92307692, ...,   0.        ,\n",
       "         13.        ,   8.        ],\n",
       "       [205.        ,  32.        ,   5.4375    , ...,   0.        ,\n",
       "         29.        ,  13.        ],\n",
       "       [ 48.        ,  10.        ,   3.9       , ...,   0.        ,\n",
       "         10.        ,   4.        ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:32:30.760653Z",
     "start_time": "2025-02-20T08:32:30.754347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.api.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.regularizers import l2\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "    \"\"\"Train the model with all features\"\"\"\n",
    "    # Split the data\n",
    "    X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(\n",
    "        data['processed_text'],\n",
    "        text_features,\n",
    "        data['label'],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=15000,\n",
    "        min_df=2,\n",
    "        ngram_range=(1, 3),\n",
    "        stop_words='english'\n",
    "    )\n",
    "\n",
    "    # Transform the text data to feature vectors\n",
    "    X1_train_tfidf = tfidf.fit_transform(X1_train).toarray()  # Convert to dense matrix\n",
    "    X1_test_tfidf = tfidf.transform(X1_test).toarray()  # Use the same TF-IDF fit on test set\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X1_train_tfidf_scaled = scaler.fit_transform(X1_train_tfidf)\n",
    "    X1_test_tfidf_scaled = scaler.transform(X1_test_tfidf)\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    feature_weight = 1  # Adjust this value to give more weight to additional features\n",
    "\n",
    "    X2_train_scaled = scaler_features.fit_transform(X2_train) * feature_weight\n",
    "    X2_test_scaled = scaler_features.transform(X2_test) * feature_weight\n",
    "\n",
    "    X_train_combined = np.hstack((X1_train_tfidf_scaled, X2_train_scaled))\n",
    "    X_test_combined = np.hstack((X1_test_tfidf_scaled, X2_test_scaled))\n",
    "\n",
    "\n",
    "    # Normalize labels to be between 0 and 4\n",
    "    y_train_scaled = y_train - 1\n",
    "    y_test_scaled = y_test - 1\n",
    "\n",
    "    # Build the MLP model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_combined.shape[1],)))\n",
    "    model.add(Dense(512, activation='relu'))  # Hidden layer\n",
    "    model.add(Dropout(0.1))  # Dropout\n",
    "    model.add(Dense(256, activation='relu'))  # Hidden layer\n",
    "    model.add(Dropout(0.1))  # Dropout\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))  # Dropout\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))  # Dropout\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.002)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train_combined, y_train_scaled, epochs=100, batch_size=32, callbacks=[early_stopping],\n",
    "              validation_split=0.2)\n",
    "    print(\"Model training completed.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = model.evaluate(X_test_combined, y_test_scaled)\n",
    "    print(f\"Accuracy: {accuracy[1] * 100:.2f}%\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_combined)\n",
    "    y_pred_classes = y_pred.argmax(axis=-1) + 1  # Convert to original label scale (1-5)\n",
    "\n",
    "    # Calculate accuracy or other metrics if needed\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return model, accuracy, tfidf\n"
   ],
   "id": "9adc2036b2e48ed0",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:32:38.635445Z",
     "start_time": "2025-02-20T08:32:32.491232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model, metrics, tfidf = train_model(df)\n",
    "\n",
    "print(metrics)"
   ],
   "id": "8740d0f588b81400",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/100\n",
      "\u001B[1m166/166\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.2578 - loss: 1.6397 - val_accuracy: 0.3261 - val_loss: 1.5269\n",
      "Epoch 2/100\n",
      "\u001B[1m166/166\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.4841 - loss: 1.2665 - val_accuracy: 0.3358 - val_loss: 1.5353\n",
      "Epoch 3/100\n",
      "\u001B[1m166/166\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8034 - loss: 0.5672 - val_accuracy: 0.3238 - val_loss: 1.9193\n",
      "Epoch 4/100\n",
      "\u001B[1m166/166\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.9351 - loss: 0.2175 - val_accuracy: 0.3396 - val_loss: 2.6961\n",
      "Model training completed.\n",
      "\u001B[1m52/52\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.3396 - loss: 1.5199\n",
      "Accuracy: 32.65%\n",
      "\u001B[1m52/52\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\n",
      "Test Accuracy: 32.65%\n",
      "0.3265060240963855\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:28:52.241054Z",
     "start_time": "2025-02-20T08:28:41.901598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "validation = pd.read_csv(\"work-data/val.csv\")\n",
    "\n",
    "validation['processed_text'] = validation['text'].apply(preprocess_text)\n",
    "validation\n"
   ],
   "id": "9c5b6f6d17e6ecaa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        id  label  \\\n",
       "0     6447      5   \n",
       "1     8503      2   \n",
       "2     2594      5   \n",
       "3     6482      2   \n",
       "4     5685      2   \n",
       "...    ...    ...   \n",
       "1773  1055      4   \n",
       "1774  4880      2   \n",
       "1775  6134      2   \n",
       "1776   615      4   \n",
       "1777   464      3   \n",
       "\n",
       "                                                                                                                                                                                               text  \\\n",
       "0                                                                                                                  At least it 's a fairly impressive debut from the director , Charles Stone III .   \n",
       "1                                                                                                                                                                              Bland but harmless .   \n",
       "2     Muccino , who directed from his own screenplay , is a canny crowd pleaser , and The Last Kiss ... provides more than enough sentimental catharsis for a satisfying evening at the multiplex .   \n",
       "3                                                                                                 This mistaken-identity picture is so film-culture referential that the final product is a ghost .   \n",
       "4                                 Laconic and very stilted in its dialogue , this indie flick never found its audience , probably because it 's extremely hard to relate to any of the characters .   \n",
       "...                                                                                                                                                                                             ...   \n",
       "1773                                                                        Bravo for history rewritten , and for the uncompromising knowledge that the highest power of all is the power of love .   \n",
       "1774                                                                                                    The movie keeps coming back to the achingly unfunny Phonce and his several silly subplots .   \n",
       "1775                                                                                                                              Director David Fincher and writer David Koepp ca n't sustain it .   \n",
       "1776                                                                                                               A rich tale of our times , very well told with an appropriate minimum of means .   \n",
       "1777                                                                                                                            Heaven is a haunting dramatization of a couple 's moral ascension .   \n",
       "\n",
       "                                                                                                                                                                         processed_text  \n",
       "0                                                                                                                     at least it is a fairly impressive debut from the director person  \n",
       "1                                                                                                                                                                   person but harmless  \n",
       "2     muccino who directed from his own screenplay is a canny crowd pleaser and the last kiss provides more than enough sentimental catharsis for a satisfying evening at the multiplex  \n",
       "3                                                                                       this mistaken identity picture is so film culture referential that the final product is a ghost  \n",
       "4                            person and very stilted in its dialogue this indie flick never found its audience probably because it is extremely hard to relate to any of the characters  \n",
       "...                                                                                                                                                                                 ...  \n",
       "1773                                                                bravo for history rewritten and for the uncompromising knowledge that the highest power of all is the power of love  \n",
       "1774                                                                                          the movie keeps coming back to the achingly unfunny phonce and his several silly subplots  \n",
       "1775                                                                                                                                director person and writer person ca not sustain it  \n",
       "1776                                                                                                       a rich tale of our times very well told with an appropriate minimum of means  \n",
       "1777                                                                                                                  heaven is a haunting dramatization of a couple is moral ascension  \n",
       "\n",
       "[1778 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6447</td>\n",
       "      <td>5</td>\n",
       "      <td>At least it 's a fairly impressive debut from the director , Charles Stone III .</td>\n",
       "      <td>at least it is a fairly impressive debut from the director person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8503</td>\n",
       "      <td>2</td>\n",
       "      <td>Bland but harmless .</td>\n",
       "      <td>person but harmless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2594</td>\n",
       "      <td>5</td>\n",
       "      <td>Muccino , who directed from his own screenplay , is a canny crowd pleaser , and The Last Kiss ... provides more than enough sentimental catharsis for a satisfying evening at the multiplex .</td>\n",
       "      <td>muccino who directed from his own screenplay is a canny crowd pleaser and the last kiss provides more than enough sentimental catharsis for a satisfying evening at the multiplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6482</td>\n",
       "      <td>2</td>\n",
       "      <td>This mistaken-identity picture is so film-culture referential that the final product is a ghost .</td>\n",
       "      <td>this mistaken identity picture is so film culture referential that the final product is a ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5685</td>\n",
       "      <td>2</td>\n",
       "      <td>Laconic and very stilted in its dialogue , this indie flick never found its audience , probably because it 's extremely hard to relate to any of the characters .</td>\n",
       "      <td>person and very stilted in its dialogue this indie flick never found its audience probably because it is extremely hard to relate to any of the characters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>1055</td>\n",
       "      <td>4</td>\n",
       "      <td>Bravo for history rewritten , and for the uncompromising knowledge that the highest power of all is the power of love .</td>\n",
       "      <td>bravo for history rewritten and for the uncompromising knowledge that the highest power of all is the power of love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>4880</td>\n",
       "      <td>2</td>\n",
       "      <td>The movie keeps coming back to the achingly unfunny Phonce and his several silly subplots .</td>\n",
       "      <td>the movie keeps coming back to the achingly unfunny phonce and his several silly subplots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>6134</td>\n",
       "      <td>2</td>\n",
       "      <td>Director David Fincher and writer David Koepp ca n't sustain it .</td>\n",
       "      <td>director person and writer person ca not sustain it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>615</td>\n",
       "      <td>4</td>\n",
       "      <td>A rich tale of our times , very well told with an appropriate minimum of means .</td>\n",
       "      <td>a rich tale of our times very well told with an appropriate minimum of means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>464</td>\n",
       "      <td>3</td>\n",
       "      <td>Heaven is a haunting dramatization of a couple 's moral ascension .</td>\n",
       "      <td>heaven is a haunting dramatization of a couple is moral ascension</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1778 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:28:52.258981Z",
     "start_time": "2025-02-20T08:28:52.257807Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5ced2a700aacf791",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:32:22.975077Z",
     "start_time": "2025-02-20T08:32:22.658778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_val():\n",
    "    # Transform the text data to feature vectors\n",
    "    data_tfidf = tfidf.transform(validation['processed_text']).toarray()  # Convert to dense matrix\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    x1_val_scaled = scaler.fit_transform(data_tfidf)\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    feature_weight = 1.0  # Adjust this value to give more weight to additional features\n",
    "\n",
    "    text_features = get_text_stats_value(validation['processed_text'])\n",
    "\n",
    "    x2_val_scaled = scaler_features.fit_transform(text_features) * feature_weight\n",
    "    X_val_combined = np.hstack((x1_val_scaled, x2_val_scaled))\n",
    "\n",
    "    print(X_val_combined.shape)\n",
    "\n",
    "    return model.predict(X_val_combined)\n",
    "\n",
    "y_predict = predict_val()"
   ],
   "id": "cbd9eb7f9032c0af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1778, 8739)\n",
      "\u001B[1m56/56\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:32:25.961687Z",
     "start_time": "2025-02-20T08:32:25.957836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how many labels are same as predicted\n",
    "validation['predicted_label'] = y_predict.argmax(axis=-1) + 1\n",
    "correct = validation[validation['label'] == validation['predicted_label']]\n",
    "\n",
    "len(correct) / len(validation)"
   ],
   "id": "cc4c2d885e67e4d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34195725534308213"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:29:03.403397Z",
     "start_time": "2025-02-20T08:28:52.586492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_nolabel():\n",
    "    \"\"\"Predict no label and output as id and label in a csv\"\"\"\n",
    "    predict_data = pd.read_csv(\"work-data/test_nolabel.csv\")\n",
    "    predict_data['processed_text'] = predict_data['text'].apply(preprocess_text)\n",
    "\n",
    "    data_tfidf = tfidf.transform(predict_data['processed_text']).toarray()  # Convert to dense matrix\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    x1_val_scaled = scaler.fit_transform(data_tfidf)\n",
    "\n",
    "    scaler_features = StandardScaler()\n",
    "    feature_weight = 1.0  # Adjust this value to give more weight to additional features\n",
    "\n",
    "    text_features = get_text_stats_value(predict_data['processed_text'])\n",
    "\n",
    "    x2_val_scaled = scaler_features.fit_transform(text_features) * feature_weight\n",
    "    X_val_combined = np.hstack((x1_val_scaled, x2_val_scaled))\n",
    "\n",
    "    print(X_val_combined.shape)\n",
    "\n",
    "    y_predict = model.predict(X_val_combined)\n",
    "\n",
    "    predict_data['label'] = y_predict.argmax(axis=-1) + 1\n",
    "\n",
    "    predict_data[['id', 'label']].to_csv(\"work-data/test_label.csv\", index=False)\n",
    "\n",
    "predict_nolabel()"
   ],
   "id": "8d84cd7bd4846d06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1779, 8739)\n",
      "\u001B[1m56/56\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n"
     ]
    }
   ],
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
